# Migration to Snowpipe Streaming REST API Only

## Summary

This application now **exclusively uses** the Snowpipe Streaming v2 REST API for data ingestion. The direct insert fallback method has been removed.

## What Changed

### Files Modified

1. **`main.py`**
   - Removed `thermal_direct_insert` import
   - Now only uses `SnowpipeStreamingClient` from `thermal_streaming_client.py`
   - Updated initialization to call `discover_ingest_host()` and `open_channel()`
   - Updated shutdown to call `close_channel()`

### Files Removed

1. **`thermal_direct_insert.py`** - Direct SQL INSERT method (no longer needed)

### Files Unchanged

- `thermal_streaming_client.py` - REST API client (fully functional)
- `thermal_sensor.py` - Sensor reading logic
- `snowflake_jwt_auth.py` - PAT authentication
- `test_connection.py` - Already tests REST API only
- `README.md` - Already focuses on REST API
- `requirements.txt` - Dependencies remain the same

## Why This Change?

The Snowpipe Streaming v2 REST API is Snowflake's **recommended high-performance ingestion method** with:

- ‚úÖ **Lower latency** (5-10 seconds vs 15-30+ seconds)
- ‚úÖ **Higher throughput** (up to 10 GB/s per table)
- ‚úÖ **Better scalability** (serverless architecture)
- ‚úÖ **Automatic micro-batching** (Snowflake handles optimization)
- ‚úÖ **No warehouse costs** (ingestion is free)

The direct insert method was a fallback that used SQL INSERT statements, which:
- ‚ùå Required a running warehouse (incurs compute costs)
- ‚ùå Had higher latency
- ‚ùå Was not the intended use case for streaming data

## What You Need to Do

### 1. Grant Permissions in Snowflake

The REST API requires the `OPERATE` privilege on the pipe:

```sql
USE ROLE ACCOUNTADMIN;
USE DATABASE DEMO;
USE SCHEMA DEMO;

-- Grant OPERATE privilege (required for Snowpipe Streaming)
GRANT OPERATE ON PIPE THERMAL_SENSOR_PIPE TO ROLE ACCOUNTADMIN;

-- Grant INSERT privilege (required for data ingestion)
GRANT INSERT ON TABLE THERMAL_SENSOR_DATA TO ROLE ACCOUNTADMIN;
```

### 2. Verify Pipe Exists

```sql
USE DATABASE DEMO;
USE SCHEMA DEMO;

-- Check if pipe exists
SHOW PIPES LIKE 'THERMAL_SENSOR_PIPE';
```

If it doesn't exist, run the setup script:

```bash
snowsql -f setup_snowflake.sql
```

### 3. Test Connection

```bash
python test_connection.py
```

Expected output:

```
======================================================================
TEST 4: Streaming Client
======================================================================
[OK] Client initialized
Discovering ingest host...
[OK] Ingest host: lxb29530.ingest.iadaax.snowflakecomputing.com
Opening channel...
[OK] Channel opened: thermal_channel_001
[OK] Streaming client ready
```

### 4. Run the Application

```bash
# With real sensors
python main.py --batch-size 100 --interval 10

# Simulation mode (no hardware)
python main.py --simulate --batch-size 10 --interval 5
```

## Troubleshooting

### Error: "ERR_ROLE_DOES_NOT_EXIST_OR_NOT_AUTHORIZED"

**Cause:** User/role doesn't have OPERATE privilege on the pipe.

**Fix:** Run the GRANT commands above in Snowflake.

### Error: "404 Not Found" when opening channel

**Cause:** Pipe doesn't exist or incorrect database/schema/pipe names.

**Fix:**
1. Verify pipe exists: `SHOW PIPES;`
2. Check config file has correct names
3. Re-run setup script if needed

### Error: "401 Unauthorized"

**Cause:** PAT token is invalid or expired.

**Fix:** Generate a new PAT:

```sql
ALTER USER THERMAL_STREAMING_USER 
  SET PROGRAMMATIC_ACCESS_TOKEN 
  ENABLED = TRUE 
  EXPIRES_IN = 90;
```

Copy the secret and update `snowflake_config.json`.

## Benefits of REST API Only

### Simplified Architecture

**Before (with fallback):**
```
Sensor ‚Üí main.py ‚Üí [Try REST API] ‚Üí [Fallback to Direct Insert] ‚Üí Snowflake
                    (2 code paths)
```

**Now (REST API only):**
```
Sensor ‚Üí main.py ‚Üí REST API ‚Üí Snowflake
         (1 code path, simpler)
```

### Better Performance

| Metric | Direct Insert | REST API |
|--------|--------------|----------|
| Latency | 15-30 seconds | 5-10 seconds |
| Throughput | Limited by warehouse | Up to 10 GB/s |
| Cost | Warehouse compute | Free ingestion |
| Scalability | Manual scaling | Auto-scaling |

### Cleaner Codebase

- **-238 lines** of code removed (`thermal_direct_insert.py`)
- **Fewer dependencies** (no need for `snowflake.connector` SQL logic)
- **Single ingestion path** (easier to maintain)
- **Better error handling** (REST API provides detailed errors)

## Migration Checklist

- [x] Remove `thermal_direct_insert.py`
- [x] Update `main.py` to use REST API only
- [x] Update initialization to call `discover_ingest_host()` and `open_channel()`
- [x] Update shutdown to call `close_channel()`
- [ ] **Run SQL grants in Snowflake** (see above)
- [ ] **Test connection:** `python test_connection.py`
- [ ] **Run application:** `python main.py --simulate`
- [ ] **Verify data in Snowflake**

## Support

If you encounter issues:

1. Check `thermal_streaming.log` for detailed errors
2. Run `python test_connection.py` to diagnose
3. Review `TROUBLESHOOTING.md` for common issues
4. Ensure you've run the SQL grants above

## Next Steps

1. **Production deployment:**
   - Set up systemd service (see README.md)
   - Configure log rotation
   - Set up monitoring

2. **Optimize performance:**
   - Adjust `--batch-size` and `--interval` for your use case
   - Monitor Snowflake ingestion metrics
   - Review `BEST_PRACTICES.md`

3. **Security:**
   - Rotate PAT tokens regularly
   - Use dedicated service account
   - Monitor access logs

---

**The application is now simpler, faster, and follows Snowflake best practices! üöÄ**

